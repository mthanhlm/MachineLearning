{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('california_housing_train.csv')\n",
    "num_cols = data.shape[1]\n",
    "num_rows = data.shape[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hóa dữ liệu: scale dữ liệu, format kích thước dữ liệu\n",
    "normalized_data_dict = {}\n",
    "for i in range(num_cols-1):\n",
    "    column_name = data.columns[i]\n",
    "    column_data = data[column_name]\n",
    "    normalized_data = (column_data - np.mean(column_data))/ np.std(column_data) # type: ignore\n",
    "    normalized_data_dict[column_name] = normalized_data\n",
    "\n",
    "X = pd.DataFrame(normalized_data_dict)\n",
    "X = np.hstack((np.ones((num_rows, 1)), X))\n",
    "Y = data[data.columns[-1]].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:17<00:00,  2.57s/it]\n",
      "100%|██████████| 30/30 [01:14<00:00,  2.50s/it]\n",
      "100%|██████████| 30/30 [01:11<00:00,  2.39s/it]\n",
      " 30%|███       | 9/30 [00:21<00:50,  2.40s/it]"
     ]
    }
   ],
   "source": [
    "# Hàm Gradient descent\n",
    "def gradient_descent(X, y, theta, alpha, iteration):\n",
    "    m = len(y)\n",
    "    J_history = np.zeros((iteration, 1))\n",
    "\n",
    "    for i in tqdm(range(iteration)):\n",
    "        theta = theta - (alpha/m) * (X.T @ (X @ theta - y))\n",
    "        J_history[i] = compute_cost(X, y, theta)\n",
    "        # print('Đang train iteration thứ :',i+1)\n",
    "\n",
    "    return theta, J_history\n",
    "\n",
    "# Hàm tính Cost J \n",
    "def compute_cost(X, y, theta):\n",
    "    m = len(y)\n",
    "    J = (1/(2*m)) * np.sum((X @ theta - y)**2)\n",
    "    return J\n",
    "\n",
    "# Khởi tạo giá trị theta ,learning rate(alpha) và số vòng lặp\n",
    "theta = np.zeros((X.shape[1], 1))\n",
    "learning_rates = [0.001, 0.003, 0.009, 0.01]\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "iteration = 30\n",
    "\n",
    "# Chạy Gradient Descent\n",
    "for i, alpha in enumerate(learning_rates):\n",
    "    theta, J_history = gradient_descent(X, Y, theta, alpha, iteration)\n",
    "    plt.plot(J_history, color=colors[i], label=f'Learning rate: {alpha}')\n",
    "\n",
    "# Vẽ biểu đồ J ở các giá trị learning rate khác nhau sau khi chạy hết các vòng lặp.\n",
    "# plt.plot(range(len(J_history)), J_history,color = 'black')\n",
    "plt.title('Tác động của số lượng vòng lặp và các giá trị của learning rate vào hàm cost')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Hàm tính theta bằng phương pháp normal equation\n",
    "def normal_equation(X, y):\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "# Kiểm chứng theta bằng phương pháp normal equation\n",
    "theta_ne = normal_equation(X, Y)\n",
    "print(f'Theta from gradient descent: {theta}')\n",
    "print(f'Theta from normal equation: {theta_ne}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
